{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61611edc",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5e3a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Reading from: /workspaces/MAP_Hackathon_2026_G4/src/data/raw_full\n",
      "ðŸŽ¯ Saving to: /workspaces/MAP_Hackathon_2026_G4/src/data/processed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "# Config\n",
    "RAW_DIR = os.path.join(os.getcwd(), \"data\", \"raw_full\")\n",
    "PROCESSED_DIR = os.path.join(os.getcwd(), \"data\", \"processed\")\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# Threshold: Drop columns with > 50% missing values\n",
    "MISSING_THRESHOLD = 0.5 \n",
    "\n",
    "# Helper Function: Reduce Memory Usage \n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"\n",
    "    Converts float64 -> float32 and int64 -> int32 to save memory.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif col_type == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "            \n",
    "    return df\n",
    "\n",
    "print(f\"ðŸ“‚ Reading from: {RAW_DIR}\")\n",
    "print(f\"ðŸŽ¯ Saving to: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775b749",
   "metadata": {},
   "source": [
    "## Load All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d458f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš“ Anchor File: DEMO_J.xpt\n",
      "   Initial Shape: (9254, 46)\n",
      "   Memory (Optimized): 1.62 MB\n"
     ]
    }
   ],
   "source": [
    "# 1. Get all file paths\n",
    "all_files = glob.glob(os.path.join(RAW_DIR, \"*.xpt\")) + glob.glob(os.path.join(RAW_DIR, \"*.XPT\"))\n",
    "demo_path = next((f for f in all_files if \"DEMO\" in os.path.basename(f).upper()), None)\n",
    "\n",
    "if not demo_path:\n",
    "    raise FileNotFoundError(\"CRITICAL: DEMO file not found!\")\n",
    "\n",
    "print(f\"âš“ Anchor File: {os.path.basename(demo_path)}\")\n",
    "df_master = pd.read_sas(demo_path)\n",
    "\n",
    "# Optimize Anchor immediately\n",
    "df_master['SEQN'] = df_master['SEQN'].astype('int32')\n",
    "df_master = optimize_dtypes(df_master)\n",
    "base_seqns = set(df_master['SEQN'])\n",
    "\n",
    "print(f\"   Initial Shape: {df_master.shape}\")\n",
    "print(f\"   Memory (Optimized): {df_master.memory_usage().sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1734b5",
   "metadata": {},
   "source": [
    "## Memory-Optimized Iterative Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d76cc681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting iterative merge with compression...\n",
      "   ... Merged 10 files. Shape: (17529, 757), Mem: 50.69 MB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "dropped_vars_count = 0\n",
    "\n",
    "print(\"ðŸš€ Starting iterative merge with compression...\")\n",
    "\n",
    "for f in all_files:\n",
    "    if f == demo_path: continue\n",
    "    \n",
    "    # Garbage collection before heavy lifting\n",
    "    gc.collect()\n",
    "    \n",
    "    try:\n",
    "        # A. Read file\n",
    "        df_temp = pd.read_sas(f)\n",
    "        \n",
    "        # B. Check SEQN\n",
    "        if 'SEQN' not in df_temp.columns:\n",
    "            del df_temp\n",
    "            continue\n",
    "            \n",
    "        # Optimize SEQN immediately for matching\n",
    "        df_temp['SEQN'] = df_temp['SEQN'].astype('int32')\n",
    "        \n",
    "        # C. Row Filter (Keep only valid participants)\n",
    "        df_temp = df_temp[df_temp['SEQN'].isin(base_seqns)]\n",
    "        \n",
    "        if df_temp.empty:\n",
    "            del df_temp\n",
    "            continue\n",
    "            \n",
    "        # D. Column Filter (Drop empty columns)\n",
    "        cols_to_keep = ['SEQN']\n",
    "        # Calculate missingness\n",
    "        # Note: We compute this on the FILTERED rows, which is more accurate\n",
    "        missing = df_temp.isnull().mean()\n",
    "        valid_cols = missing[missing <= MISSING_THRESHOLD].index.tolist()\n",
    "        \n",
    "        # Ensure we don't duplicate columns already in master (except SEQN)\n",
    "        new_cols = [c for c in valid_cols if c not in df_master.columns or c == 'SEQN']\n",
    "        \n",
    "        if len(new_cols) <= 1: # Only SEQN remains\n",
    "            del df_temp\n",
    "            continue\n",
    "            \n",
    "        # Keep only valid, new columns\n",
    "        df_temp = df_temp[new_cols]\n",
    "        \n",
    "        # E. COMPRESS DATA (The Fix)\n",
    "        df_temp = optimize_dtypes(df_temp)\n",
    "        \n",
    "        # F. Merge\n",
    "        df_master = pd.merge(df_master, df_temp, on='SEQN', how='left')\n",
    "        \n",
    "        # G. Cleanup\n",
    "        del df_temp\n",
    "        count += 1\n",
    "        \n",
    "        if count % 10 == 0:\n",
    "            mem_usage = df_master.memory_usage().sum() / 1024**2\n",
    "            print(f\"   ... Merged {count} files. Shape: {df_master.shape}, Mem: {mem_usage:.2f} MB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error {os.path.basename(f)}: {e}\")\n",
    "        # Ensure cleanup even on error\n",
    "        if 'df_temp' in locals(): del df_temp\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175bda3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536df982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "# === Configuration ===\n",
    "RAW_DIR = os.path.join(os.getcwd(), \"data\", \"raw_full\")\n",
    "TEMP_DIR = os.path.join(os.getcwd(), \"data\", \"temp_parts\") # Intermediate storage\n",
    "PROCESSED_DIR = os.path.join(os.getcwd(), \"data\", \"processed\")\n",
    "\n",
    "# Create/Reset directories\n",
    "if os.path.exists(TEMP_DIR):\n",
    "    shutil.rmtree(TEMP_DIR)\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "MISSING_THRESHOLD = 0.5 \n",
    "\n",
    "# === Helper: Reduce Memory Usage ===\n",
    "def optimize_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 1: PRE-PROCESS TO DISK\n",
    "# ==========================================\n",
    "print(\"ðŸš€ PHASE 1: Pre-processing files to disk...\")\n",
    "\n",
    "# 1. Setup Anchor (DEMO)\n",
    "all_files = glob.glob(os.path.join(RAW_DIR, \"*.xpt\")) + glob.glob(os.path.join(RAW_DIR, \"*.XPT\"))\n",
    "demo_path = next((f for f in all_files if \"DEMO\" in os.path.basename(f).upper()), None)\n",
    "\n",
    "print(f\"âš“ Loading Anchor: {os.path.basename(demo_path)}\")\n",
    "df_demo = pd.read_sas(demo_path)\n",
    "df_demo['SEQN'] = df_demo['SEQN'].astype('int32')\n",
    "base_seqns = set(df_demo['SEQN'])  # The master list of valid IDs\n",
    "\n",
    "# Save DEMO immediately and clear memory\n",
    "df_demo = optimize_dtypes(df_demo)\n",
    "df_demo.to_pickle(os.path.join(TEMP_DIR, \"000_DEMO.pkl\"))\n",
    "print(f\"   -> Saved Anchor. Rows: {len(df_demo)}\")\n",
    "del df_demo\n",
    "gc.collect()\n",
    "\n",
    "# 2. Process all other files\n",
    "saved_count = 0\n",
    "for i, f in enumerate(all_files):\n",
    "    if f == demo_path: continue\n",
    "    fname = os.path.basename(f)\n",
    "    \n",
    "    try:\n",
    "        # Load ONE file\n",
    "        df_temp = pd.read_sas(f)\n",
    "        \n",
    "        # Validation\n",
    "        if 'SEQN' not in df_temp.columns:\n",
    "            del df_temp\n",
    "            continue\n",
    "            \n",
    "        # Filter Rows (Keep only valid participants)\n",
    "        df_temp['SEQN'] = df_temp['SEQN'].astype('int32')\n",
    "        df_temp = df_temp[df_temp['SEQN'].isin(base_seqns)]\n",
    "        \n",
    "        if df_temp.empty:\n",
    "            del df_temp\n",
    "            continue\n",
    "\n",
    "        # Filter Columns (Drop empty ones)\n",
    "        # We calculate missingness on the valid rows only\n",
    "        missing = df_temp.isnull().mean()\n",
    "        valid_cols = missing[missing <= MISSING_THRESHOLD].index.tolist()\n",
    "        \n",
    "        # Must keep SEQN\n",
    "        if 'SEQN' not in valid_cols: valid_cols.append('SEQN')\n",
    "        \n",
    "        # If the file has no useful data, skip it\n",
    "        if len(valid_cols) <= 1: \n",
    "            del df_temp\n",
    "            continue\n",
    "            \n",
    "        # Keep only valid cols & Compress\n",
    "        df_temp = df_temp[valid_cols]\n",
    "        df_temp = optimize_dtypes(df_temp)\n",
    "        \n",
    "        # SAVE to disk and delete from RAM\n",
    "        save_name = f\"{i:03d}_{fname.replace('.xpt', '').replace('.XPT', '')}.pkl\"\n",
    "        df_temp.to_pickle(os.path.join(TEMP_DIR, save_name))\n",
    "        \n",
    "        saved_count += 1\n",
    "        if saved_count % 10 == 0:\n",
    "            print(f\"   Processed {saved_count} files...\")\n",
    "            \n",
    "        del df_temp\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Skipped {fname}: {e}\")\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"âœ… Phase 1 Complete. {saved_count} clean files saved to disk.\")\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 2: MERGE FROM DISK\n",
    "# ==========================================\n",
    "print(\"\\nðŸš€ PHASE 2: Merging cleaned files...\")\n",
    "\n",
    "# Load the anchor back\n",
    "df_master = pd.read_pickle(os.path.join(TEMP_DIR, \"000_DEMO.pkl\"))\n",
    "\n",
    "# Get all temp files (excluding DEMO)\n",
    "temp_files = sorted(glob.glob(os.path.join(TEMP_DIR, \"*.pkl\")))\n",
    "temp_files = [f for f in temp_files if \"000_DEMO\" not in f]\n",
    "\n",
    "for i, f in enumerate(temp_files):\n",
    "    try:\n",
    "        # Load small clean chunk\n",
    "        df_part = pd.read_pickle(f)\n",
    "        \n",
    "        # Remove duplicate columns if any (keep Master's version)\n",
    "        cols_to_use = df_part.columns.difference(df_master.columns).tolist()\n",
    "        cols_to_use.append('SEQN')\n",
    "        \n",
    "        # Merge\n",
    "        df_master = pd.merge(df_master, df_part[cols_to_use], on='SEQN', how='left')\n",
    "        \n",
    "        # Cleanup\n",
    "        del df_part\n",
    "        if i % 10 == 0:\n",
    "            print(f\"   Merged {i+1}/{len(temp_files)}... Current Shape: {df_master.shape}\")\n",
    "            gc.collect()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Merge Error on {os.path.basename(f)}: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 3: FINAL OUTPUT\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"ðŸŽ‰ GRAND TOTAL: {df_master.shape}\")\n",
    "\n",
    "# Save final\n",
    "final_pkl = os.path.join(PROCESSED_DIR, \"nhanes_final.pkl\")\n",
    "df_master.to_pickle(final_pkl)\n",
    "print(f\"ðŸ’¾ Saved Pickle: {final_pkl}\")\n",
    "\n",
    "# Clean up temp files\n",
    "shutil.rmtree(TEMP_DIR)\n",
    "print(\"ðŸ§¹ Temp files cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f905f",
   "metadata": {},
   "source": [
    "## Final Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df5dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"ðŸŽ‰ Merge Complete!\")\n",
    "print(f\"ðŸ“Š Final Shape: {df_master.shape}\")\n",
    "print(f\"ðŸ—‘ï¸ Total Variables Dropped (Pre-filter): {dropped_vars_count}\")\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = os.path.join(PROCESSED_DIR, \"nhanes_2017_2018_filtered.csv\")\n",
    "df_master.to_csv(csv_path, index=False)\n",
    "print(f\"ðŸ’¾ Saved CSV to: {csv_path}\")\n",
    "\n",
    "# Save as Pickle (Recommended for Python workflows - reads/writes much faster)\n",
    "pkl_path = os.path.join(PROCESSED_DIR, \"nhanes_2017_2018_filtered.pkl\")\n",
    "df_master.to_pickle(pkl_path)\n",
    "print(f\"ðŸ’¾ Saved Pickle to: {pkl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af7a88",
   "metadata": {},
   "source": [
    "## Calculate Missing Rates (Memory Safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import gc  # Garbage Collection\n",
    "\n",
    "# === Configuration ===\n",
    "# Path to your raw XPT files\n",
    "RAW_DIR = os.path.join(os.getcwd(), \"data\", \"raw_full\")\n",
    "MISSING_THRESHOLD = 0.5  # 50% threshold\n",
    "\n",
    "print(f\"ðŸ“‚ Scanning files in: {RAW_DIR}\")\n",
    "all_files = glob.glob(os.path.join(RAW_DIR, \"*.xpt\")) + glob.glob(os.path.join(RAW_DIR, \"*.XPT\"))\n",
    "print(f\"ðŸ” Found {len(all_files)} files. Calculating missing rates iteratively...\")\n",
    "\n",
    "var_missing_rates = {}\n",
    "\n",
    "for i, f in enumerate(all_files):\n",
    "    try:\n",
    "        # Read one file at a time\n",
    "        df_temp = pd.read_sas(f)\n",
    "        \n",
    "        # Calculate missing rate for each variable in this file\n",
    "        # This returns a Series like: {'SEQN': 0.0, 'LBXGLU': 0.1, ...}\n",
    "        rates = df_temp.isnull().mean()\n",
    "        \n",
    "        # Store in our master dictionary\n",
    "        # Note: If a variable appears in multiple files (like SEQN), this will overwrite with the latest.\n",
    "        # Since variables are mostly unique to specific tables, this is fine for a general overview.\n",
    "        var_missing_rates.update(rates.to_dict())\n",
    "        \n",
    "        # Free memory immediately\n",
    "        del df_temp\n",
    "        gc.collect()\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"   Processed {i + 1}/{len(all_files)} files...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error reading {os.path.basename(f)}: {e}\")\n",
    "\n",
    "# Convert dictionary to Series for your plotting code\n",
    "missing_series = pd.Series(var_missing_rates)\n",
    "\n",
    "print(f\"\\nâœ… Analysis Complete!\")\n",
    "print(f\"   Total Variables Analyzed: {len(missing_series)}\")\n",
    "print(f\"   Variables to Keep (< {MISSING_THRESHOLD*100}% missing): {len(missing_series[missing_series <= MISSING_THRESHOLD])}\")\n",
    "print(f\"   Variables to Drop (> {MISSING_THRESHOLD*100}% missing): {len(missing_series[missing_series > MISSING_THRESHOLD])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e757be",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b97318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kept vs dropped\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(missing_series * 100, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.axvline(MISSING_THRESHOLD * 100, color='red', linestyle='--', label=f'Cutoff ({MISSING_THRESHOLD*100}%)')\n",
    "plt.title('Distribution of Missing Data % Across All Variables')\n",
    "plt.xlabel('% Missing')\n",
    "plt.ylabel('Number of Variables')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00a05c",
   "metadata": {},
   "source": [
    "## Filter by Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§¹ Starting Filtration...\")\n",
    "\n",
    "# 1. Calculate missing rates\n",
    "missing_series = df_master.isnull().mean()\n",
    "\n",
    "# 2. Identify keep vs drop\n",
    "keep_cols = missing_series[missing_series <= MISSING_THRESHOLD].index\n",
    "drop_cols = missing_series[missing_series > MISSING_THRESHOLD].index\n",
    "\n",
    "print(f\"   Total Variables: {len(missing_series)}\")\n",
    "print(f\"   Keep (<= {MISSING_THRESHOLD*100}% missing): {len(keep_cols)}\")\n",
    "print(f\"   Drop (> {MISSING_THRESHOLD*100}% missing): {len(drop_cols)}\")\n",
    "\n",
    "# 3. Create filtered DataFrame\n",
    "df_clean = df_master[keep_cols].copy()\n",
    "\n",
    "# 4. Save\n",
    "save_path = os.path.join(PROCESSED_DIR, \"nhanes_2017_2018_filtered.csv\")\n",
    "# Use pickle for faster IO in Python, or CSV for compatibility\n",
    "df_clean.to_csv(save_path, index=False)\n",
    "df_clean.to_pickle(os.path.join(PROCESSED_DIR, \"nhanes_2017_2018_filtered.pkl\"))\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Saved clean data to: {save_path}\")\n",
    "print(f\"ðŸ“Š Final Clean Shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d91295",
   "metadata": {},
   "source": [
    "## Visualize Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38476618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we kept vs dropped\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(missing_series * 100, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.axvline(MISSING_THRESHOLD * 100, color='red', linestyle='--', label=f'Cutoff ({MISSING_THRESHOLD*100}%)')\n",
    "plt.title('Distribution of Missing Data % Across All Variables')\n",
    "plt.xlabel('% Missing')\n",
    "plt.ylabel('Number of Variables')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coi_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
